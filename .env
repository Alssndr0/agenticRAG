# OpenAI API settings
OPENAI_API_KEY="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"
OPENAI_API_URL=http://127.0.0.1:1234/v1/chat/completions
OPENAI_MODEL=lmstudio-community/granite-vision-3.2-2b
OPENAI_MAX_TOKENS=1200 
VLM_PROMPT="If this image is a table or a chart, provide a detailed explanation \
of the insight it wants to convey and extract all the relevant data into a markdown format. \
If it is not a table or a chart, just output 'generic image', only these two words, without any additional text."
# Input folder
INPUT_FOLDER=data/original
# Output folder
OUTPUT_FOLDER=data/chunked
# Document summarise output folder
SUMMARISE_OUTPUT_FOLDER=data/enhanced
# Document summarise output file
SUMMARISE_OUTPUT_FILE=data/enhanced/document_summaries.txt
# Summarise chunk output file
SUMMARISE_CHUNK_OUTPUT_FILE=data/enhanced/chunk_context_summaries.txt
# Chunk size
CHUNK_SIZE=400
# Embed model id
EMBED_MODEL_ID=Alibaba-NLP/gte-Qwen2-7B-instruct
# Output files for chunks and metadata
CHUNKS_FILE=data/chunked/chunks.json
METADATA_FILE=data/chunked/metadata.json
# Summarise dococument prompt
SUMMARISE_DOCUMENT_PROMPT="Give a short succinct description of the overall document for the purposes of improving search retrieval. \
Answer only with a very succinct summary and nothing else."
# Summary document input words 
SUMMARISE_DOCUMENT_INPUT_WORDS=1000
# Summarisation model
SUMMARISE_MODEL=gpt-4o-mini
# Summarise chunk prompt
SUMMARISE_CHUNK_PROMPT="Provide only a very short, succinct context summary for the target text to improve its searchability. \ 
Start with 'This chunk details..."