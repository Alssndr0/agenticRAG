# OpenAI API settings
OPENAI_API_KEY="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"
OPENAI_API_URL=http://127.0.0.1:1234/v1/chat/completions
OPENAI_MODEL=lmstudio-community/granite-vision-3.2-2b
OPENAI_MAX_TOKENS=1200 
VLM_PROMPT="If this image is a table or a chart, provide a detailed explanation \
of the insight it wants to convey and extract all the relevant data into a markdown format. \
If it is not a table or a chart, just output 'generic image', only these two words, without any additional text."

# Input/Output folders
INPUT_FOLDER=data/test
OUTPUT_FOLDER=data/chunked

# Chunking settings
CHUNK_SIZE=400
MIN_WORDS=200

# Embedding model
EMBED_MODEL_ID=Alibaba-NLP/gte-Qwen2-7B-instruct

# Output files
EXTRACTED_CHUNKS_FILE=data/chunked/extracted_chunks.json
EMBEDDINGS_FILE=data/chunked/embeddings.npz

# Legacy output files (for backward compatibility)
CHUNKS_FILE=data/chunked/chunks.json
METADATA_FILE=data/chunked/metadata.json

# Summarization settings
SUMMARISE_MODEL=gpt-4o-mini
SUMMARISE_DOCUMENT_PROMPT="Give a short succinct description of the overall document for the purposes of improving search retrieval. \
Answer only with a very succinct summary and nothing else."
SUMMARISE_DOCUMENT_INPUT_WORDS=1000
SUMMARISE_CHUNK_PROMPT="Provide only a very short, succinct context summary for the target text to improve its searchability. \ 
Start with 'This chunk details...'"