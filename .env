# OpenAI API settings
OPENAI_API_KEY="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"
OPENAI_API_URL=http://127.0.0.1:1234/v1/chat/completions
OPENAI_MODEL=lmstudio-community/granite-vision-3.2-2b
OPENAI_MAX_TOKENS=1200 
VLM_PROMPT="If this image is a table or a chart, provide a detailed explanation \
of the insight it wants to convey and extract all the relevant data into a markdown format. \
If it is not a table or a chart, just output 'generic image', only these two words, without any additional text."
# Input folder
INPUT_FOLDER=data/original
# Output folder
OUTPUT_FOLDER=data/chunked
# Chunk size
CHUNK_SIZE=400
# Embed model id
EMBED_MODEL_ID=Alibaba-NLP/gte-Qwen2-7B-instruct
# Output files for chunks and metadata
CHUNKS_FILE=data/chunked/chunks.json
METADATA_FILE=data/chunked/metadata.json
